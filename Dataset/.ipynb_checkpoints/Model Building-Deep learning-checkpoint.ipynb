{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63df514c",
   "metadata": {},
   "source": [
    "**Model Building-Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea8f441",
   "metadata": {},
   "source": [
    "LSTM, DNN (Deep Neural Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32be3be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\lxy03\\appdata\\roaming\\python\\python312\\site-packages (2.19.0)\n",
      "Requirement already satisfied: keras-tuner in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.14.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.3)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.5-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\lxy03\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (74.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lxy03\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsmodels) (1.15.2)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lxy03\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.9.2)\n",
      "Requirement already satisfied: namex in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lxy03\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lxy03\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\lxy03\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\lxy03\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\lxy03\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow --upgrade keras-tuner statsmodels pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53556e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SGD\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m train_data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain_data.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m val_data = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mval_data.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     20\u001b[39m test_data = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mtest_data.csv\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lxy03\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lxy03\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lxy03\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lxy03\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lxy03\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'train_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "val_data = pd.read_csv('val_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "X_train = train_data.drop(columns=['Social_Anxiety_Category'])\n",
    "y_train = train_data['Social_Anxiety_Category']\n",
    "X_val = val_data.drop(columns=['Social_Anxiety_Category'])\n",
    "y_val = val_data['Social_Anxiety_Category']\n",
    "X_test = test_data.drop(columns=['Social_Anxiety_Category'])\n",
    "y_test = test_data['Social_Anxiety_Category']\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_val = le.transform(y_val)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "# One-hot encoding the labels\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_val = to_categorical(y_val, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data for deep learning models (e.g., CNN, LSTM)\n",
    "# For DNN, reshape it to be 2D: (samples, features)\n",
    "X_train_dnn = X_train\n",
    "X_val_dnn = X_val\n",
    "X_test_dnn = X_test\n",
    "\n",
    "# Reshape the data for 1D CNN\n",
    "X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # (samples, features, channels)\n",
    "X_val_cnn = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Reshape the data for LSTM\n",
    "X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_val_lstm = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6e487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Model Performance Evaluation ----------------------\n",
    "\n",
    "def evaluate_deep_learning_model(model, model_name, x_train, y_train, x_test, y_test, history=None):\n",
    "    # Model summary\n",
    "    print(f\"\\n{'='*40}\\n{' '*10}{model_name} Model Summary\\n{'='*40}\")\n",
    "    model.summary()\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print(f\"\\n{'='*40}\\nTest Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\\n\")\n",
    "\n",
    "    # ------------------- Classification Report -------------------\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)  # Get the predicted class indices\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    classificationreport = classification_report(y_true, y_pred_classes, output_dict=True)\n",
    "    report_df = pd.DataFrame(classificationreport).transpose()\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    display(report_df.round(2))\n",
    "\n",
    "    # ------------------- Confusion Matrix -------------------\n",
    "    confusionmatrix = confusion_matrix(y_true, y_pred_classes)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(confusionmatrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, annot_kws={\"size\": 12})\n",
    "    plt.title(f'{model_name} - Confusion Matrix', fontsize=16, pad=20)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.show()\n",
    "\n",
    "    # ------------------- Accuracy and Loss Plot -------------------\n",
    "    if history:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Accuracy plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue', linewidth=2)\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange', linewidth=2)\n",
    "        plt.title(f'{model_name} - Accuracy', fontsize=16)\n",
    "        plt.xlabel('Epochs', fontsize=12)\n",
    "        plt.ylabel('Accuracy', fontsize=12)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "        # Loss plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss', color='orange', linewidth=2)\n",
    "        plt.title(f'{model_name} - Loss', fontsize=16)\n",
    "        plt.xlabel('Epochs', fontsize=12)\n",
    "        plt.ylabel('Loss', fontsize=12)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return test_accuracy, test_loss, classificationreport, confusionmatrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42138e55",
   "metadata": {},
   "source": [
    "1.Deep Neural Network (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dnn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=X_train_dnn.shape[1:]),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# DNN Model\n",
    "dnn_model = build_dnn()\n",
    "history_dnn = dnn_model.fit(X_train_dnn, y_train, batch_size=64, epochs=20, \n",
    "                            validation_data=(X_val_dnn, y_val), shuffle=True, \n",
    "                            callbacks=[early_stopping])\n",
    "\n",
    "dnn_accuracy, dnn_loss, dnn_classificationreport, dnn_confusionmatrix = evaluate_deep_learning_model(\n",
    "    dnn_model, \"DNN Model\", X_train_dnn, y_train, X_test_dnn, y_test, history_dnn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bfd061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import keras_tuner as kt\n",
    "import pandas as pd\n",
    "import ace_tools as tools  # Ensure ace_tools is available to display the DataFrame\n",
    "\n",
    "# Define the function to build the DNN model with SGD optimizer\n",
    "def build_tuned_dnn_model(hp):\n",
    "    model = Sequential([\n",
    "        layers.Flatten(input_shape=X_train_dnn.shape[1:]),\n",
    "\n",
    "        # Tunable Dense layers\n",
    "        # Number of hidden layers can be tuned\n",
    "        for i in range(hp.Int('num_layers', 1, 3)):  \n",
    "            model.add(layers.Dense(hp.Int(f'units_{i}', min_value=128, max_value=1024, step=128), activation='relu'))\n",
    "            model.add(layers.Dropout(hp.Float(f'dropout_{i}', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # Output layer\n",
    "        model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    ])\n",
    "\n",
    "    # Create the SGD optimizer with tunable learning rate and momentum\n",
    "    optimizer = SGD(\n",
    "        learning_rate=hp.Float('learning_rate', min_value=0.001, max_value=0.01, sampling='log'),\n",
    "        momentum=hp.Float('momentum', min_value=0.7, max_value=0.9, step=0.1)\n",
    "    )\n",
    "    \n",
    "    # Compile the model with the SGD optimizer\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Initialize the Keras Tuner Hyperband\n",
    "tuner_dnn = kt.Hyperband(\n",
    "    build_tuned_dnn_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=10,  # Increase max epochs for a more thorough search\n",
    "    hyperband_iterations=2,  # Increased iterations for a more refined search\n",
    "    directory='dnn_tuning',\n",
    "    project_name='dnn_tuning_project'\n",
    ")\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner_dnn.search(X_train_dnn, y_train, epochs=10, validation_data=(X_val_dnn, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Get the best model after hyperparameter tuning\n",
    "best_dnn_model = tuner_dnn.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Train the best DNN model with the tuned hyperparameters\n",
    "history_dnn_tuned = best_dnn_model.fit(\n",
    "    X_train_dnn, y_train, batch_size=64, epochs=20,  # Increase epochs for final training\n",
    "    validation_data=(X_val_dnn, y_val), shuffle=True,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Extract the best hyperparameters from the tuner\n",
    "best_trial = tuner_dnn.oracle.get_best_trials(num_trials=1)[0]  # Getting the best trial\n",
    "best_hyperparameters = best_trial.hyperparameters.values\n",
    "\n",
    "# Create a dictionary to hold the hyperparameters and their values\n",
    "hyperparameters_dict = {key: best_hyperparameters[key] for key in best_hyperparameters}\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame for better visualization\n",
    "hyperparameters_df = pd.DataFrame(list(hyperparameters_dict.items()), columns=['Hyperparameter', 'Value'])\n",
    "\n",
    "# Display the DataFrame in a nice table format\n",
    "tools.display_dataframe_to_user(name=\"Best Hyperparameters\", dataframe=hyperparameters_df)\n",
    "\n",
    "# Evaluate the tuned DNN model (ensure 'evaluate_deep_learning_model' function is available)\n",
    "dnntuned_accuracy, dnntuned_loss, dnntuned_classificationreport, dnntuned_confusionmatrix = evaluate_deep_learning_model(\n",
    "    best_dnn_model, \"Tuned DNN Model\", X_train_dnn, y_train, X_test_dnn, y_test, history_dnn_tuned\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe19d7d1",
   "metadata": {},
   "source": [
    "2.Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aea64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Calculate the maximum kernel size based on input sequence length\n",
    "    max_kernel_size = min(4, X_train_cnn.shape[1])\n",
    "\n",
    "    # Hyperparameters for Conv1D layers\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters_1', min_value=32, max_value=128, step=32), \n",
    "        kernel_size=hp.Int('kernel_size_1', min_value=2, max_value=max_kernel_size, step=1),\n",
    "        activation='relu', \n",
    "        input_shape=(X_train_cnn.shape[1], 1),\n",
    "        kernel_initializer='he_normal', \n",
    "        kernel_regularizer='l2',\n",
    "        padding='same'))  # Use 'same' padding\n",
    "    model.add(MaxPooling1D(2))\n",
    "    \n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters_2', min_value=64, max_value=256, step=64),\n",
    "        kernel_size=hp.Int('kernel_size_2', min_value=2, max_value=max_kernel_size, step=1),\n",
    "        activation='relu',\n",
    "        padding='same'))  # Use 'same' padding\n",
    "    model.add(MaxPooling1D(2))\n",
    "\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters_3', min_value=128, max_value=512, step=128),\n",
    "        kernel_size=hp.Int('kernel_size_3', min_value=2, max_value=max_kernel_size, step=1),\n",
    "        activation='relu',\n",
    "        padding='same'))  # Use 'same' padding\n",
    "    model.add(MaxPooling1D(2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(rate=hp.Float('dropout_rate', min_value=0.3, max_value=0.7, step=0.1)))\n",
    "    \n",
    "    # Dense layer with tunable units\n",
    "    model.add(Dense(units=hp.Int('dense_units', min_value=128, max_value=512, step=128), activation='relu'))\n",
    "    \n",
    "    # Output layer with 3 units for the 3 classes\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    # Compile the model with a tunable learning rate\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Initialize Keras Tuner's Hyperband\n",
    "tuner_cnn = kt.Hyperband(\n",
    "    build_cnn_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=5,  # Reduced number of epochs per trial\n",
    "    hyperband_iterations=1,  # Only 1 iteration for fewer trials\n",
    "    directory='cnn_tuning',\n",
    "    project_name='cnn_tuning_project'\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner_cnn.search(X_train_cnn, y_train, epochs=5, validation_data=(X_val_cnn, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Get the best model from the tuner\n",
    "best_cnn_model = tuner_cnn.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Train the best model\n",
    "history_cnn_tuned = best_cnn_model.fit(X_train_cnn, y_train, batch_size=64, epochs=20, \n",
    "                                        validation_data=(X_val_cnn, y_val), shuffle=True, \n",
    "                                        callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the tuned model\n",
    "cnntuned_accuracy,cnntuned_loss, cnntuned_classificationreport, cnntuned_confusionmatrix = evaluate_deep_learning_model(\n",
    "    best_cnn_model, \"Tuned CNN Model\", X_train_cnn, y_train, X_test_cnn, y_test, history_cnn_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c38df7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Calculate the maximum kernel size based on input sequence length\n",
    "    max_kernel_size = min(4, X_train_cnn.shape[1])\n",
    "\n",
    "    # Hyperparameters for Conv1D layers\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters_1', min_value=32, max_value=128, step=32), \n",
    "        kernel_size=hp.Int('kernel_size_1', min_value=2, max_value=max_kernel_size, step=1),\n",
    "        activation='relu', \n",
    "        input_shape=(X_train_cnn.shape[1], 1),\n",
    "        kernel_initializer='he_normal', \n",
    "        kernel_regularizer='l2',\n",
    "        padding='same'))  # Use 'same' padding\n",
    "    model.add(MaxPooling1D(2))\n",
    "    \n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters_2', min_value=64, max_value=256, step=64),\n",
    "        kernel_size=hp.Int('kernel_size_2', min_value=2, max_value=max_kernel_size, step=1),\n",
    "        activation='relu',\n",
    "        padding='same'))  # Use 'same' padding\n",
    "    model.add(MaxPooling1D(2))\n",
    "\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters_3', min_value=128, max_value=512, step=128),\n",
    "        kernel_size=hp.Int('kernel_size_3', min_value=2, max_value=max_kernel_size, step=1),\n",
    "        activation='relu',\n",
    "        padding='same'))  # Use 'same' padding\n",
    "    model.add(MaxPooling1D(2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(rate=hp.Float('dropout_rate', min_value=0.3, max_value=0.7, step=0.1)))\n",
    "    \n",
    "    # Dense layer with tunable units\n",
    "    model.add(Dense(units=hp.Int('dense_units', min_value=128, max_value=512, step=128), activation='relu'))\n",
    "    \n",
    "    # Output layer with 3 units for the 3 classes\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    # Compile the model with a tunable learning rate\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Initialize Keras Tuner's Hyperband\n",
    "tuner_cnn = kt.Hyperband(\n",
    "    build_cnn_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=5,  # Reduced number of epochs per trial\n",
    "    hyperband_iterations=1,  # Only 1 iteration for fewer trials\n",
    "    directory='cnn_tuning',\n",
    "    project_name='cnn_tuning_project'\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner_cnn.search(X_train_cnn, y_train, epochs=5, validation_data=(X_val_cnn, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Get the best model from the tuner\n",
    "best_cnn_model = tuner_cnn.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Train the best model\n",
    "history_cnn_tuned = best_cnn_model.fit(X_train_cnn, y_train, batch_size=64, epochs=20, \n",
    "                                        validation_data=(X_val_cnn, y_val), shuffle=True, \n",
    "                                        callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the tuned model\n",
    "cnntuned_accuracy,cnntuned_loss, cnntuned_classificationreport, cnntuned_confusionmatrix = evaluate_deep_learning_model(\n",
    "    best_cnn_model, \"Tuned CNN Model\", X_train_cnn, y_train, X_test_cnn, y_test, history_cnn_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a20f6e4",
   "metadata": {},
   "source": [
    "Compare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
